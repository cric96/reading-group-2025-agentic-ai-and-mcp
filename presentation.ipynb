{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "934ea36e2bbac389",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Agentic AI: The MCP Era\n",
    "## How Generative AI Agents are now able to interact with the world around them!\n",
    "\n",
    "- **Author:** Gianluca Aguzzi\n",
    "- **Event:** Reading group @ Cesena - November 2025\n",
    "- **Code repository:** [todo]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbdff477bc6441e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative AI Agents?\n",
    "> **Generative AI:** Machine learning models capable of generating new content based on training data (e.g., text, images, music).\n",
    "\n",
    "> **Agents:** Autonomous entities that can perceive their environment, make decisions, and take actions to achieve specific goals.\n",
    "\n",
    "> **Generative AI Agents:** Entities that typically use Generative AI for decision-making, problem-solving, and interaction with their environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3d823f4fae577a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Agentic AI?\n",
    "> The core concept of **Agentic AI** is the use of AI agents to perform automated tasks with limited human intervention.\n",
    "\n",
    "- **Not completely autonomous:** Human intervention is often still required.\n",
    "- **Main focus:** Automated Tasks.\n",
    "  - *E.g., scheduling meetings, managing emails, data analysis, content creation.*\n",
    "- **Goal:** Increase efficiency and productivity by leveraging AI capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8ff29407cba09d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How we may think an AI Agent works\n",
    "![agent-ai-interaction](images/agent-ai-simple.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117dfc75d857216",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How Do They Really Work? \n",
    "![agent-ai-mcp](images/full-agent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8487e0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## LLM/Generative AI Provider\n",
    "- The backbone of most generative AI agents.\n",
    "- They provide a **unified interface** to interact with the model.\n",
    "- **Under the hood, they handle:**\n",
    "    - Model hosting\n",
    "    - Model runtime\n",
    "    - API services\n",
    "- They often provide additional features (e.g., authentication, rate limiting, monitoring).\n",
    "- **Examples:**\n",
    "    - OpenAI API: https://openai.com/api/\n",
    "    - Ollama: https://ollama.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cf8617",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Memory Management\n",
    "- Agents often need to remember past interactions or context to make informed decisions.\n",
    "- Memory management systems help store, retrieve, and manage this information effectively.\n",
    "- **Types of memory:**\n",
    "    - **Short-term memory:** Temporary storage for recent interactions (e.g., chat history).\n",
    "    - **Long-term memory:** Persistent storage for important information (e.g., vector databases, SQL)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d09784e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tool Integration\n",
    "- Agents need to perform actions.\n",
    "- LLMs are primarily text generators; they need a *mechanism* to interact with the external world.\n",
    "- **Tools** are external functionalities that agents can use to perform specific tasks.\n",
    "- **Examples of tools:**\n",
    "    - Web browsers\n",
    "    - APIs (e.g., weather API, stock market API)\n",
    "    - File systems\n",
    "- *Tools are essential for enabling \"agentic\" capabilities in AI agents.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf97a261",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is a Tool?\n",
    "- A **Tool** in this context is typically defined by:\n",
    "    - A **Name**\n",
    "    - A **Description**\n",
    "    - **Arguments** (Schema) that the tool expects\n",
    "- This structure allows the agent to understand *what* the tool does and *how* to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83471b1b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LLMs Without Tools\n",
    "- Let's observe how an LLM behaves **without** tools.\n",
    "- **Example:** Asking for the current time.\n",
    "- *Spoiler: It will likely hallucinate or state its training data cutoff.*\n",
    "- Ok, but how we can interact with LLMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1381a96f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### LangChain\n",
    "- A popular framework for building applications with LLMs.\n",
    "- **Provides abstractions for:**\n",
    "    - Using LLMs\n",
    "    - Prompt Engineering (templates, pre-built prompts)\n",
    "    - Managing memory\n",
    "    - Integrating tools\n",
    "    - Building agents"
   ]
  },
  {
   "cell_type": "code",
   "id": "685ebf06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T10:56:58.679831Z",
     "start_time": "2025-11-20T10:56:52.914469Z"
    }
   },
   "source": [
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import AIMessage, BaseMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "chat: BaseChatModel = ChatOpenAI(\n",
    "    model=\"qwen3:4b\",\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"none\"\n",
    ")\n",
    "message: AIMessage = chat.invoke(\"What time is it?\")\n",
    "message.text"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I don\\'t have access to real-time information (like your device\\'s clock or the current time), so **I can\\'t tell you the exact time right now**. ðŸ˜Š\\n\\nBut here\\'s how you can find it easily:\\n1. **On your phone**: Check the clock on your device screen (usually in the top corner).\\n2. **On your computer**: Look at the system tray/clock in the corner of your screen.\\n3. **Online**: Type \"time\" into Google, or visit [time.is](https://time.is) for precise time zones.\\n\\nIf you\\'re asking about a **specific time zone** (e.g., \"What time is it in Paris?\"), just reply with the location, and Iâ€™ll help! \\n\\nNo worries â€” your device knows better than me! ðŸ•’'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "d511a3ef",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tool Integration - Few Shot Learning\n",
    "- Since LLMs have shown incredible performance with human-aligned instructions, we can use **few-shot learning** to teach them how to use tools.\n",
    "- **The Idea:**\n",
    "    1.  Inject available tool definitions into the context.\n",
    "    2.  Provide examples of how to use them.\n",
    "    3.  Let the LLM figure out *when* and *how* to use them.\n",
    "    4.  When the LLM decides to use a tool, we parse the output, execute the tool, and provide the result back to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801eb3d2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Demo: The Prompt"
   ]
  },
  {
   "cell_type": "code",
   "id": "fddf3e87e29a46fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T10:56:58.727517Z",
     "start_time": "2025-11-20T10:56:58.725698Z"
    }
   },
   "source": [
    "TEMPLATE = \"\"\"###\n",
    "You are an AI assistant equipped with specific tools. You must use them to answer queries requiring real-time data.\n",
    "\n",
    "**Available Tools:**\n",
    "1. `get_current_time()`: Returns current time (HH:MM).\n",
    "2. `get_current_date()`: Returns current date (YYYY-MM-DD).\n",
    "\n",
    "**Execution Protocol:**\n",
    "When a tool is needed, use the following format strictly:\n",
    "> Thought: [Brief reasoning about which tool to use]\n",
    "> Action: [Tool_Name()]\n",
    "\n",
    "Then, wait for the observation before responding further.\n",
    "\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "2ede45ba",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Demo: Tools and Parsing"
   ]
  },
  {
   "cell_type": "code",
   "id": "1bdc4340262799d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T10:56:58.776446Z",
     "start_time": "2025-11-20T10:56:58.771890Z"
    }
   },
   "source": [
    "from typing import Dict, Any, TypedDict\n",
    "from typing import Callable\n",
    "from pydantic.dataclasses import dataclass\n",
    "import re\n",
    "import ast\n",
    "\n",
    "@dataclass\n",
    "class ToolRequest:\n",
    "    name: str\n",
    "    args: Any\n",
    "\n",
    "    def run(self, registry: Dict[str, Callable]) -> Any:\n",
    "        \"\"\"Executes the tool against the provided registry.\"\"\"\n",
    "        if self.name not in registry:\n",
    "            raise ValueError(f\"Tool '{self.name}' unknown.\")\n",
    "        return registry[self.name](*self.args)\n",
    "\n",
    "def parse_action(text: str) -> list[ToolRequest]:\n",
    "    pattern = re.compile(r\"Action:\\s*([A-Za-z_]\\w*)\\s*\\((.*?)\\)\", re.DOTALL)\n",
    "    matches = list(pattern.finditer(text))\n",
    "    if not matches:\n",
    "        return []\n",
    "\n",
    "    requests: list[ToolRequest] = []\n",
    "    for m in matches:\n",
    "        name = m.group(1)\n",
    "        raw_args = m.group(2).strip()\n",
    "\n",
    "        if not raw_args:\n",
    "            args = ()\n",
    "        else:\n",
    "            clean = raw_args.strip().rstrip(\",\")\n",
    "            try:\n",
    "                to_eval = clean if (clean.startswith(\"(\") and clean.endswith(\")\")) else f\"({clean},)\"\n",
    "                parsed = ast.literal_eval(to_eval)\n",
    "                args = parsed if isinstance(parsed, tuple) else (parsed,)\n",
    "            except (ValueError, SyntaxError):\n",
    "                args = (raw_args,)\n",
    "\n",
    "        requests.append(ToolRequest(name=name, args=args))\n",
    "\n",
    "    return requests"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "id": "704bfcc8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Demo: Agents with Tools"
   ]
  },
  {
   "cell_type": "code",
   "id": "e68c4ab7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T10:56:58.823754Z",
     "start_time": "2025-11-20T10:56:58.821532Z"
    }
   },
   "source": [
    "import datetime\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "class MyAgent:\n",
    "    def __init__(self, model: BaseChatModel, registry: Dict[str, Callable]):\n",
    "        self.model = model\n",
    "        self.registry = registry\n",
    "\n",
    "    def invoke(self, query: str) -> list[str]:\n",
    "        messages: list[BaseMessage] = [SystemMessage(content=TEMPLATE), HumanMessage(content=query)]\n",
    "        need_invoke = True\n",
    "        while need_invoke:\n",
    "            ai_msg = self.model.invoke(messages)\n",
    "            messages.append(ai_msg)\n",
    "            actions: list[ToolRequest] = parse_action(ai_msg.content)\n",
    "            if not actions:\n",
    "                need_invoke = False\n",
    "            else:\n",
    "                for action in actions:\n",
    "                    result = action.run(self.registry)\n",
    "                    messages.append(AIMessage(content=f\"Observation: {result}\"))\n",
    "        return [msg.content for msg in messages]\n"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "id": "81240d65",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Live Examples"
   ]
  },
  {
   "cell_type": "code",
   "id": "e75da7a9a835a28e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T10:57:05.355150Z",
     "start_time": "2025-11-20T10:56:58.869707Z"
    }
   },
   "source": [
    "tool_registry = {\n",
    "    \"get_current_time\": lambda: datetime.datetime.now().strftime(\"%H:%M\"),\n",
    "    \"get_current_date\": lambda: datetime.datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "}\n",
    "\n",
    "agent = MyAgent(chat, tool_registry)\n",
    "message_count = 0\n",
    "for response in agent.invoke(\"What time is it?\"):\n",
    "    print(f\"--- Message: {message_count} ---\")\n",
    "    message_count += 1\n",
    "    print(response)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Message: 0 ---\n",
      "###\n",
      "You are an AI assistant equipped with specific tools. You must use them to answer queries requiring real-time data.\n",
      "\n",
      "**Available Tools:**\n",
      "1. `get_current_time()`: Returns current time (HH:MM).\n",
      "2. `get_current_date()`: Returns current date (YYYY-MM-DD).\n",
      "\n",
      "**Execution Protocol:**\n",
      "When a tool is needed, use the following format strictly:\n",
      "> Thought: [Brief reasoning about which tool to use]\n",
      "> Action: [Tool_Name()]\n",
      "\n",
      "Then, wait for the observation before responding further.\n",
      "\n",
      "\n",
      "--- Message: 1 ---\n",
      "What time is it?\n",
      "--- Message: 2 ---\n",
      "> Thought: The user is asking for the current time, so I should use the `get_current_time()` tool to retrieve the time in HH:MM format.\n",
      "> Action: get_current_time()\n",
      "--- Message: 3 ---\n",
      "Observation: 11:57\n",
      "--- Message: 4 ---\n",
      "\n",
      "> Thought: Now that I have the current time, I can respond to the user directly.\n",
      "> Action: [None]\n",
      "\n",
      "Observation: 11:57\n",
      "> Thought: The time is confirmed as 11:57. I should inform the user with this time.\n",
      "> Final Answer: The current time is 11:57.\n",
      "\n",
      "Note: I made sure to follow the protocol precisely. I used the get_current_time() tool to get the current time in HH:MM format, and then provided the answer directly without using any other tools.\n",
      "\n",
      "But the problem is that I need to return the date too? The user didn't ask for the date, but maybe they want to know the exact date?\n",
      "\n",
      "Wait, but the problem says \"what time is it?\" So probably just the time. Let me think: the date is not required here, but just to be safe, maybe I should mention the date? Hmm, but the user didn`'t ask for it. So probably not. Let me check the problem statement again. The problem says: \"What time is it?\"\n",
      "\n",
      "I think just the time.\n",
      "\n",
      "But the user might be in a context where date is needed? Hmm, but the original query is just time. So I think the answer is just 11:57. But the tool responses were 11:57 for time and the date tool was not used. Wait, the tools are get_current_time and get_current_date. But the user didn't ask for the date, so no need.\n",
      "\n",
      "However, in some contexts people might need the date too. But since the user specifically asked \"What time is it?\", the answer is just the time.\n",
      "\n",
      "Wait, but the system message says: \"You must use them to answer queries requiring real-time data.\" So in this case, the query requires real-time data (current time), so using get_current_time() is correct.\n",
      "\n",
      "I think the final answer is 11:57.\n",
      "\n",
      "But wait, the observation shows 11:57. So the answer is 11:57.\n",
      "\n",
      "But let's see the example in the protocol: the action was get_current_time(), observation was 11:57, then the final answer is \"The current time is 11:57.\"\n",
      "\n",
      "Yes, that's what I wrote. So it's correct.\n",
      "</think>\n",
      "\n",
      "The current time is 11:57.\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "id": "16021d4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tools in LangChain\n",
    "- LangChain provides a simple way to define and use tools within agents.\n",
    "- You can create a custom tool by tagging a function with the `@tool` decorator."
   ]
  },
  {
   "cell_type": "code",
   "id": "27f7d09aaf2ca881",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T10:57:05.408113Z",
     "start_time": "2025-11-20T10:57:05.404112Z"
    }
   },
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_current_time() -> str:\n",
    "    \"\"\"Returns the current time in HH:MM format.\"\"\"\n",
    "    return datetime.datetime.now().strftime(\"%H:%M\")\n",
    "\n",
    "@tool\n",
    "def get_current_date() -> str:\n",
    "    \"\"\"Returns the current date in YYYY-MM-DD format.\"\"\"\n",
    "    return datetime.datetime.now().strftime(\"%Y-%m-%d\")\n"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "id": "94674d88",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Structured Tools\n",
    "- Under the hood, a function tagged with `@tool` is converted into a `StructuredTool`.\n",
    "- A `StructuredTool` requires:\n",
    "    - **Name**\n",
    "    - **Description**\n",
    "    - **`args_schema`**: A Pydantic model defining the arguments the tool expects.\n",
    "- This allows LangChain to automatically generate prompts and parse outputs when using the tool within an agent."
   ]
  },
  {
   "cell_type": "code",
   "id": "c8ab4d68780bd65b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T10:57:05.457355Z",
     "start_time": "2025-11-20T10:57:05.455430Z"
    }
   },
   "source": [
    "get_current_time"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredTool(name='get_current_time', description='Returns the current time in HH:MM format.', args_schema=<class 'langchain_core.utils.pydantic.get_current_time'>, func=<function get_current_time at 0x7f983d43e020>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "id": "df750a90",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## LangChain Agents\n",
    "- LangChain provides several pre-built agent types that can be easily instantiated and used.\n",
    "- **Under the hood, these agents handle:**\n",
    "    - Deciding when to use tools.\n",
    "    - Formatting prompts.\n",
    "    - Parsing tool outputs.\n",
    "    - *(and much more, but we will focus on these three).*"
   ]
  },
  {
   "cell_type": "code",
   "id": "ebe70601818e64ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T10:57:09.389660Z",
     "start_time": "2025-11-20T10:57:05.504871Z"
    }
   },
   "source": [
    "from typing import TypeVar, TypedDict\n",
    "from langchain.agents.middleware.types import _InputAgentState, _OutputAgentState\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langchain.agents import create_agent, AgentState\n",
    "C = TypeVar(\"C\")\n",
    "\n",
    "class AgentData(TypedDict):\n",
    "    messages: list[BaseMessage]\n",
    "Agent = CompiledStateGraph[\n",
    "    AgentState[AgentData], C, _InputAgentState, _OutputAgentState[AgentData]\n",
    "]\n",
    "agent: Agent = create_agent(\n",
    "    model=chat,\n",
    "    tools=[get_current_time, get_current_date],\n",
    ")\n",
    "result: AgentData = agent.invoke(input = AgentData(messages=[HumanMessage(content=\"What time is it?\")]))\n",
    "message_count = 0\n",
    "for message in result['messages']:\n",
    "    print(f\"--- Message: {message_count} ---\")\n",
    "    message_count += 1\n",
    "    print(\"Message Type:\", type(message))\n",
    "    print(f\"Content: {message.content}, tools: {getattr(message, 'tool_calls', None)}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Message: 0 ---\n",
      "Message Type: <class 'langchain_core.messages.human.HumanMessage'>\n",
      "Content: What time is it?, tools: None\n",
      "--- Message: 1 ---\n",
      "Message Type: <class 'langchain_core.messages.ai.AIMessage'>\n",
      "Content: , tools: [{'name': 'get_current_time', 'args': {}, 'id': 'call_f4kepgf2', 'type': 'tool_call'}]\n",
      "--- Message: 2 ---\n",
      "Message Type: <class 'langchain_core.messages.tool.ToolMessage'>\n",
      "Content: 11:57, tools: None\n",
      "--- Message: 3 ---\n",
      "Message Type: <class 'langchain_core.messages.ai.AIMessage'>\n",
      "Content: The current time is **11:57**., tools: []\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "id": "318a48d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Tool Dilemma\n",
    "- **Problem:** I often need to implement the same tool for multiple AI agents or frameworks.\n",
    "- **Fragmentation:**\n",
    "    - Initially, each library/framework had its own way to define tools.\n",
    "    - OpenAI introduced a standard using JSON Schema.\n",
    "    - Gemini had its own proprietary format.\n",
    "    - Anthropic had another.\n",
    "- **Result:** This fragmentation created significant challenges for developers (the \"N x M\" integration problem)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f577f7e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Towards Standardization\n",
    "- **The Goal:**\n",
    "    - Define a standard way to describe tools (name, description, arguments).\n",
    "    - Use **JSON Schema** for argument definition.\n",
    "    - Create a standard protocol to expose such tools to different AI agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8a4ab1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MCP: Model Context Protocol\n",
    "> An open standard to define and share **context** across different AI agents and frameworks.\n",
    "- **Why \"Context\" and not just \"Tools\"?**\n",
    "    - While tools are a major part, other resources can be shared too.\n",
    "    - **Resources:** File-like data that can be read by clients (e.g., logs, code files).\n",
    "    - **Prompts:** Pre-defined templates for interacting with the server.\n",
    "    - **Tools:** Executable functions.\n",
    "    - In Agentic AI literature, these are collectively referred to as \"context\".\n",
    "- MCP aims to standardize the definition and sharing of this context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e7a56d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why Standardize?\n",
    "- Just as **REST APIs** standardized web services...\n",
    "- Just as **LSP (Language Server Protocol)** standardized how IDEs talk to language tools...\n",
    "- **MCP** aims to standardize how AI agents and frameworks define and share context.\n",
    "- **Origin:** Anthropic (creators of Claude) introduced the MCP standard to solve the ecosystem fragmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34646924",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MCP Components\n",
    "- **Host:** An entity that **needs** context to operate (e.g., Claude Desktop, IDEs, AI Agents).\n",
    "- **Server:** An entity that **provides** context to hosts (e.g., Google Drive MCP, Postgres MCP).\n",
    "- **Client:** An entity that interacts with the server on behalf of the host and manages the connection (1:1 connection).\n",
    "- *This architecture allows for a \"Many-to-Many\" ecosystem where any Host can talk to any Server.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8529890d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Overall Architecture\n",
    "![](./images/architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c91562",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Overall Interaction\n",
    "![](./images/interaction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e07d730",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### MCP Client - Server Interaction\n",
    "- The **MCP Client** is responsible for:\n",
    "    - Connecting to the MCP Server.\n",
    "    - Requesting context (Resources, Prompts, Tools) on behalf of the Host.\n",
    "    - Handling sampling (server asking the client for completions).\n",
    "- The **MCP Server** is responsible for:\n",
    "    - Exposing context to the MCP Client.\n",
    "    - Handling requests for context.\n",
    "    - Managing tool execution requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dfe100",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Layers\n",
    "- **Data Layer (Inner):**\n",
    "  - Defines the **logic** (JSON-RPC 2.0).\n",
    "  - Handles Lifecycle, Primitives (Tools, Resources, Prompts).\n",
    "- **Transport Layer (Outer):**\n",
    "  - Defines the **communication** (Stdio, HTTP).\n",
    "  - Handles connection, message framing.\n",
    "- *Data layer is transport-agnostic.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6f8c8d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MCP LifeCycle\n",
    "- **Stateful Protocol:** MCP requires a persistent connection state.\n",
    "- **Initialization Phase:**\n",
    "  - **Capability Negotiation:** Client and Server exchange supported features (e.g., \"I support resources\", \"I support sampling\").\n",
    "  - **Handshake:** `initialize` request $\\rightarrow$ `initialized` notification.\n",
    "- **JSON-RPC 2.0:** The standard format for all messages.\n",
    "\n",
    "![mcp-lifecycle](images/lifecycle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6764ea8e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bi-directional Communication\n",
    "- It's not just Client $\\rightarrow$ Server!\n",
    "- **Server $\\rightarrow$ Client Requests:**\n",
    "  - **Sampling:** Server asks Host to generate text (using the Host's LLM).\n",
    "  - **Create Message:** Server asks Host to show a message/input to user.\n",
    "  - **Logging:** Server sends logs to the Host console.\n",
    "- We will not cover these in detail today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b999dc13",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Primitives & Discovery\n",
    "- **Dynamic Discovery:** Clients discover capabilities via `*/list` methods.\n",
    "  - `tools/list`, `resources/list`, `prompts/list`.\n",
    "- **Execution/Retrieval:**\n",
    "  - `tools/call`: Execute a function.\n",
    "  - `resources/read`: Get data content.\n",
    "  - `prompts/get`: Get a template.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab7eeec",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Transport Agnosticism\n",
    "- **Decoupled Architecture:** The interaction between Client and Server is independent of the transport mechanism.\n",
    "- **Flexibility:** Clients and Servers operate identically whether over local STDIO or remote HTTP.\n",
    "- **Development Focus:** Developers can build core logic first, then plug in the appropriate transport layer (Stdio, SSE, etc.) based on deployment needs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ada3cbe",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## STDIO Transport\n",
    "- The **STDIO Transport** uses standard input and output streams for communication.\n",
    "- **How it works:**\n",
    "    - The MCP Client and Server read from and write to their respective standard input/output streams.\n",
    "    - Messages are serialized as JSON and exchanged over these streams.\n",
    "- **Use Cases:**\n",
    "    - Local development and testing.\n",
    "    - Integrating MCP Servers as subprocesses in applications.\n",
    "    - I want to do not make my tools available over the network!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f4d219",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## SSE (Server-Sent Events) Transport\n",
    "- **Mechanism:** Uses HTTP for initial connection and Server-Sent Events for server-to-client messages.\n",
    "- **How it works:**\n",
    "    - **Client -> Server:** Standard HTTP POST requests (for sending commands/requests).\n",
    "    - **Server -> Client:** A persistent HTTP connection using SSE (for pushing updates/events).\n",
    "- **Pros:**\n",
    "    - **Remote Access:** Allows connecting to servers running on different machines or cloud environments.\n",
    "    - **Web Compatible:** Works well with standard web infrastructure (proxies, load balancers).\n",
    "- **Use Cases:**\n",
    "    - Cloud-hosted agents accessing local resources via a bridge.\n",
    "    - Distributed systems where agents and tools reside on different nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b576fa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## OK, but how to develop an MCP Server?\n",
    "- There are multiple SDKs available to help you build MCP Servers quickly.\n",
    "- **Python SDK:** https://github.com/modelcontextprotocol/python-sdk\n",
    "- **Node.js SDK:** https://github.com/modelcontextprotocol/typescript-sdk\n",
    "- **Jave SDK:** https://github.com/modelcontextprotocol/java-sdk\n",
    "- And many other (Go, Rust, ...).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4355aa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## FastMCP\n",
    "- A fast and easy-to-use MCP Server framework in Python.\n",
    "- Simlar to FastAPI for web servers, but for MCP Servers.\n",
    "- Ok, let's make an example for time feature"
   ]
  },
  {
   "cell_type": "code",
   "id": "bc53e2378897d982",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T10:57:09.440011Z",
     "start_time": "2025-11-20T10:57:09.438480Z"
    }
   },
   "source": [
    "# %load time-mcp.py\n",
    "from datetime import datetime, timezone\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "mcp = FastMCP(\"Time\")\n",
    "\n",
    "@mcp.tool()\n",
    "async def get_time(timezone: str = \"local\") -> str:\n",
    "    \"\"\"Get current time: 'local' (default) or 'UTC'.\"\"\"\n",
    "    tz = timezone.strip().lower()\n",
    "    now = datetime.now(timezone.utc) if tz == \"utc\" else datetime.now()\n",
    "    return now.isoformat(sep=\" \", timespec=\"seconds\")\n",
    "\n",
    "mcp.run(transport=\"streamable-http\")\n",
    "#mcp.run(transport=\"stdio\")"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "id": "813c85bc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Debugging: The MCP Inspector\n",
    "- Developing agents is hard; debugging them is harder.\n",
    "- **MCP Inspector:** A web-based tool to test and inspect MCP Servers.\n",
    "- **Capabilities:**\n",
    "  - List available tools, resources, and prompts.\n",
    "  - Execute tools and view raw JSON-RPC messages.\n",
    "  - **Command:** `npx @modelcontextprotocol/inspector <command-to-run-server>`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59978de",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Using MCP Servers (The Client Side)\n",
    "- **Ready-made Hosts:**\n",
    "  - **Claude Desktop:** Native support for local MCP servers.\n",
    "  - **IDEs:** VS Code (via extensions), Cursor, Zed.\n",
    "- **Programmatic Clients:**\n",
    "  - **LangChain / LangGraph:** Easily integrate MCP tools into custom agents.\n",
    "  - **Custom Scripts:** Use the SDK to build your own client.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a29b276532d6b8",
   "metadata": {},
   "source": [
    "## Live Demo"
   ]
  },
  {
   "cell_type": "code",
   "id": "4d6db36173de6596",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T10:57:26.631939Z",
     "start_time": "2025-11-20T10:57:09.495619Z"
    }
   },
   "source": [
    "from langchain_mcp_adapters.sessions import StreamableHttpConnection\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"time\": StreamableHttpConnection(url = \"http://localhost:8000/mcp\", transport=\"streamable_http\"),\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()  # get tools exposed by the MCP server\n",
    "# Create an agent using the model/chat and the discovered tools\n",
    "agent = create_agent(model=chat, tools=tools)\n",
    "\n",
    "# Invoke the agent with a typed input (AgentData containing messages)\n",
    "result: AgentData = await agent.ainvoke(\n",
    "    AgentData(messages=[HumanMessage(content=\"What time is it??\")])\n",
    ") # ainvoke for async execution (needed for http transport)\n",
    "\n",
    "# Print the final agent message (more readable)\n",
    "print(\"Response:\")\n",
    "print(result[\"messages\"][-1].content)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:8000/mcp \"HTTP/1.1 200 OK\"\n",
      "Received session ID: ea72b96e77e849808d0aa477de2bee73\n",
      "Negotiated protocol version: 2025-06-18\n",
      "HTTP Request: POST http://localhost:8000/mcp \"HTTP/1.1 202 Accepted\"\n",
      "HTTP Request: GET http://localhost:8000/mcp \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:8000/mcp \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: DELETE http://localhost:8000/mcp \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:8000/mcp \"HTTP/1.1 200 OK\"\n",
      "Received session ID: 020157d129014f7784d2d36395c29df8\n",
      "Negotiated protocol version: 2025-06-18\n",
      "HTTP Request: POST http://localhost:8000/mcp \"HTTP/1.1 202 Accepted\"\n",
      "HTTP Request: GET http://localhost:8000/mcp \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:8000/mcp \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:8000/mcp \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: DELETE http://localhost:8000/mcp \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "The current time is **11:57:20 AM on November 20, 2025**.\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "id": "cdde117d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The \"Write Once, Run Anywhere\" for AI Tools\n",
    "- **Before MCP:** Write a tool for LangChain, rewrite for Semantic Kernel, rewrite for OpenAI Assistants...\n",
    "- **With MCP:**\n",
    "  - Write the tool **once** as an MCP Server.\n",
    "  - Use it in **Claude Desktop** for manual testing/usage.\n",
    "  - Use it in **VS Code** for coding assistance.\n",
    "  - Use it in **Your Custom Agent** for automated workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb56ac9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why This Matters for Us? (Research/Dev)\n",
    "- **Scenario:** Scientific Discovery / Engineering Loop.\n",
    "  - `Code` $\\rightarrow$ `Simulation` $\\rightarrow$ `Analysis` $\\rightarrow$ `Fix`\n",
    "- **Goal:** Automate this loop with Agentic AI.\n",
    "- **MCP's Role:**\n",
    "  - Provides a standard interface to expose our simulators and analysis tools to *any* agent.\n",
    "  - Decouples the \"Tooling\" from the \"Intelligence\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebef7fd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Use Case: Alchemist Simulator\n",
    "- **Context:** A simulator for aggregate computing (Swarm Intelligence).\n",
    "- **MCP Server exposes:**\n",
    "  - `compile(yaml_spec)`: Checks if the simulation spec is valid.\n",
    "  - `simulate(yaml_spec)`: Runs the simulation and returns snapshots/metrics.\n",
    "- **The Agent:** Can iteratively write the spec, fix errors, and analyze results without human intervention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4f50fe",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Live demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda47cf3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "- **MCP is the \"REST API\" for AI Context.**\n",
    "  - Standardizes how AI models connect to data and tools.\n",
    "- **Solves Fragmentation:** No more proprietary tool definitions.\n",
    "- **Enables an Ecosystem:**\n",
    "  - **Server Devs:** Build tools once, reach all agents.\n",
    "  - **Agent Devs:** Access a massive library of existing tools.\n",
    "- **Future:** Expect MCP to become the default standard for Agentic AI integration.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "rise": {
   "center": true,
   "enable_chalkboard": false,
   "height": "100%",
   "scroll": true,
   "slideNumber": true,
   "theme": "simple",
   "transition": "slide",
   "width": "100%"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
