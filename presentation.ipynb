{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "934ea36e2bbac389",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Agentic AI: The MCP Era\n",
    "## How Generative AI Agents are now able to interact with the world around them!\n",
    "\n",
    "- **Author:** Gianluca Aguzzi\n",
    "- **Event:** Reading group @ Cesena - November 2025\n",
    "- **Code repository:** [todo]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbdff477bc6441e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative AI Agents?\n",
    "> **Generative AI:** Machine learning models capable of generating new content based on training data (e.g., text, images, music).\n",
    "\n",
    "> **Agents:** Autonomous entities that can perceive their environment, make decisions, and take actions to achieve specific goals.\n",
    "\n",
    "> **Generative AI Agents:** Entities that typically use Generative AI for decision-making, problem-solving, and interaction with their environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3d823f4fae577a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Agentic AI?\n",
    "> The core concept of **Agentic AI** is the use of AI agents to perform automated tasks with limited human intervention.\n",
    "\n",
    "- **Not completely autonomous:** Human intervention is often still required.\n",
    "- **Main focus:** Automated Tasks.\n",
    "  - *E.g., scheduling meetings, managing emails, data analysis, content creation.*\n",
    "- **Goal:** Increase efficiency and productivity by leveraging AI capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8ff29407cba09d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How we may think an AI Agent works\n",
    "<img src=\"images/agent-ai-simple.png\" alt=\"images/agent-ai-simple.png\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117dfc75d857216",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How Do They Really Work? \n",
    "<img src=\"images/full-agent.png\" alt=\"images/full-agent.png\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8487e0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## LLM/Generative AI Provider\n",
    "- The backbone of most generative AI agents.\n",
    "- They provide a **unified interface** to interact with the model.\n",
    "- **Under the hood, they handle:**\n",
    "    - Model hosting\n",
    "    - Model runtime\n",
    "    - API services\n",
    "- They often provide additional features (e.g., authentication, rate limiting, monitoring).\n",
    "- **Examples:**\n",
    "    - OpenAI API: https://openai.com/api/\n",
    "    - Ollama: https://ollama.com/\n",
    "    \n",
    "<img src=\"images/provider-overview.png\" alt=\"images/llm-provider.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cf8617",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Memory Management\n",
    "- Agents often need to remember past interactions or context to make informed decisions.\n",
    "- Memory management systems help store, retrieve, and manage this information effectively.\n",
    "- **Types of memory:**\n",
    "    - **Short-term memory:** Temporary storage for recent interactions (e.g., chat history).\n",
    "    - **Long-term memory:** Persistent storage for important information (e.g., vector databases, SQL).\n",
    "\n",
    "<img src=\"images/memory.png\" alt=\"images/memory.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d09784e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tool Integration\n",
    "- Agents need to perform actions.\n",
    "- LLMs are primarily text generators; they need a *mechanism* to interact with the external world.\n",
    "- **Tools** are external functionalities that agents can use to perform specific tasks.\n",
    "- **Examples of tools:**\n",
    "    - Web browsers\n",
    "    - APIs (e.g., weather API, stock market API)\n",
    "    - File systems\n",
    "- *Tools are essential for enabling \"agentic\" capabilities in AI agents.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf97a261",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### What is a Tool?\n",
    "- A **Tool** in this context is typically defined by:\n",
    "    - A **Name**\n",
    "    - A **Description**\n",
    "    - **Arguments** (Schema) that the tool expects\n",
    "- This structure allows the agent to understand *what* the tool does and *how* to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83471b1b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LLMs Without Tools\n",
    "- Let's observe how an LLM behaves **without** tools.\n",
    "- **Example:** Asking for the current time.\n",
    "- *Spoiler: It will likely hallucinate or state its training data cutoff.*\n",
    "- Ok, but how we can interact with LLMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1381a96f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### LangChain\n",
    "- A popular framework for building applications with LLMs.\n",
    "- **Provides abstractions for:**\n",
    "    - Using LLMs\n",
    "    - Prompt Engineering (templates, pre-built prompts)\n",
    "    - Managing memory\n",
    "    - Integrating tools\n",
    "    - Building agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "685ebf06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T13:18:07.708483Z",
     "start_time": "2025-11-20T13:18:01.963078Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I don\\'t have access to real-time data, so I **can\\'t tell you the current time** (as of my last knowledge update in July 2024). ðŸ˜Š\\n\\n**Instead, hereâ€™s how you can find it quickly:**\\n\\n1. **Check your device**:  \\n   â†’ On a phone/laptop: Look at the clock in the top-right corner (most devices show time automatically).  \\n   â†’ On a watch: Check the display.  \\n\\n2. **If you need time zone help**:  \\n   Let me know your location (e.g., \"What time is it in Tokyo right now?\"), and Iâ€™ll give you the current local time!  \\n\\n*Example*:  \\n> *\"What time is it in Sydney?\"* â†’ Iâ€™ll tell you the current time in Sydney (Australia).\\n\\nJust say what you need, and Iâ€™ll help! ðŸ•’'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import AIMessage, BaseMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "chat: BaseChatModel = ChatOpenAI(\n",
    "    model=\"qwen3:4b\",\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"none\"\n",
    ")\n",
    "message: AIMessage = chat.invoke(\"What time is it?\")\n",
    "message.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d511a3ef",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tool Integration - Few Shot Learning\n",
    "- Since LLMs have shown incredible performance with human-aligned instructions, we can use **few-shot learning** to teach them how to use tools.\n",
    "> *Few-shot learning* is a technique where the model is provided with a few examples of a task to learn from (directly on the prompt), enabling it to generalize and perform similar tasks.\n",
    "- **The Idea:**\n",
    "    1.  Inject available tool definitions into the context.\n",
    "    2.  Provide examples of how to use them.\n",
    "    3.  Let the LLM figure out *when* and *how* to use them.\n",
    "    4.  When the LLM decides to use a tool, we parse the output, execute the tool, and provide the result back to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801eb3d2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Demo: The Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fddf3e87e29a46fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T13:18:07.714926Z",
     "start_time": "2025-11-20T13:18:07.713125Z"
    }
   },
   "outputs": [],
   "source": [
    "TEMPLATE = \"\"\"###\n",
    "You are an AI assistant equipped with specific tools. You must use them to answer queries requiring real-time data.\n",
    "\n",
    "**Available Tools:**\n",
    "1. `get_current_time()`: Returns current time (HH:MM).\n",
    "2. `get_current_date()`: Returns current date (YYYY-MM-DD).\n",
    "\n",
    "**Execution Protocol:**\n",
    "When a tool is needed, use the following format strictly:\n",
    "> Thought: [Brief reasoning about which tool to use]\n",
    "> Action: [Tool_Name()]\n",
    "\n",
    "Then, wait for the observation before responding further.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ede45ba",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Demo: Tools (Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1bdc4340262799d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T13:18:07.759053Z",
     "start_time": "2025-11-20T13:18:07.756844Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Any, TypedDict\n",
    "from typing import Callable\n",
    "from pydantic.dataclasses import dataclass\n",
    "import re\n",
    "import ast\n",
    "\n",
    "@dataclass\n",
    "class ToolRequest:\n",
    "    name: str\n",
    "    args: Any\n",
    "\n",
    "    def run(self, registry: Dict[str, Callable]) -> Any:\n",
    "        \"\"\"Executes the tool against the provided registry.\"\"\"\n",
    "        if self.name not in registry:\n",
    "            raise ValueError(f\"Tool '{self.name}' unknown.\")\n",
    "        return registry[self.name](*self.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fa8b8a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Demo: Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d2e8d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T13:18:07.805156Z",
     "start_time": "2025-11-20T13:18:07.803028Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_action(text: str) -> list[ToolRequest]:\n",
    "    pattern = re.compile(r\"Action:\\s*([A-Za-z_]\\w*)\\s*\\((.*?)\\)\", re.DOTALL)\n",
    "    matches = list(pattern.finditer(text))\n",
    "    if not matches:\n",
    "        return []\n",
    "\n",
    "    requests: list[ToolRequest] = []\n",
    "    for m in matches:\n",
    "        name = m.group(1)\n",
    "        raw_args = m.group(2).strip()\n",
    "\n",
    "        if not raw_args:\n",
    "            args = ()\n",
    "        else:\n",
    "            clean = raw_args.strip().rstrip(\",\")\n",
    "            try:\n",
    "                to_eval = clean if (clean.startswith(\"(\") and clean.endswith(\")\")) else f\"({clean},)\"\n",
    "                parsed = ast.literal_eval(to_eval)\n",
    "                args = parsed if isinstance(parsed, tuple) else (parsed,)\n",
    "            except (ValueError, SyntaxError):\n",
    "                args = (raw_args,)\n",
    "\n",
    "        requests.append(ToolRequest(name=name, args=args))\n",
    "\n",
    "    return requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704bfcc8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Demo: Agents with Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e68c4ab7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T13:18:07.852236Z",
     "start_time": "2025-11-20T13:18:07.849928Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "class MyAgent:\n",
    "    def __init__(self, model: BaseChatModel, registry: Dict[str, Callable]):\n",
    "        self.model = model\n",
    "        self.registry = registry\n",
    "\n",
    "    def invoke(self, query: str) -> list[str]:\n",
    "        messages: list[BaseMessage] = [SystemMessage(content=TEMPLATE), HumanMessage(content=query)]\n",
    "        need_invoke = True\n",
    "        while need_invoke:\n",
    "            ai_msg = self.model.invoke(messages)\n",
    "            messages.append(ai_msg)\n",
    "            actions: list[ToolRequest] = parse_action(ai_msg.content)\n",
    "            if not actions:\n",
    "                need_invoke = False\n",
    "            else:\n",
    "                for action in actions:\n",
    "                    result = action.run(self.registry)\n",
    "                    messages.append(AIMessage(content=f\"Observation: {result}\"))\n",
    "        return [msg.content for msg in messages]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81240d65",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Live Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e75da7a9a835a28e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T13:18:10.018293Z",
     "start_time": "2025-11-20T13:18:07.897183Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Message: 0 ---\n",
      "###\n",
      "You are an AI assistant equipped with specific tools. You must use them to answer queries requiring real-time data.\n",
      "\n",
      "**Available Tools:**\n",
      "1. `get_current_time()`: Returns current time (HH:MM).\n",
      "2. `get_current_date()`: Returns current date (YYYY-MM-DD).\n",
      "\n",
      "**Execution Protocol:**\n",
      "When a tool is needed, use the following format strictly:\n",
      "> Thought: [Brief reasoning about which tool to use]\n",
      "> Action: [Tool_Name()]\n",
      "\n",
      "Then, wait for the observation before responding further.\n",
      "\n",
      "\n",
      "--- Message: 1 ---\n",
      "What time is it?\n",
      "--- Message: 2 ---\n",
      "> Thought: The user is asking for the current time, so I should use the get_current_time() tool.\n",
      "> Action: get_current_time()\n",
      "--- Message: 3 ---\n",
      "Observation: 14:18\n",
      "--- Message: 4 ---\n",
      "\n",
      "\n",
      "> Thought: I have obtained the current time from the tool. Now I need to provide the answer to the user.\n",
      "> Action: [None]\n",
      "\n",
      "The current time is 14:18.\n",
      "</think>\n",
      "\n",
      "The current time is 14:18.\n"
     ]
    }
   ],
   "source": [
    "tool_registry = {\n",
    "    \"get_current_time\": lambda: datetime.datetime.now().strftime(\"%H:%M\"),\n",
    "    \"get_current_date\": lambda: datetime.datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "}\n",
    "\n",
    "agent = MyAgent(chat, tool_registry)\n",
    "message_count = 0\n",
    "for response in agent.invoke(\"What time is it?\"):\n",
    "    print(f\"--- Message: {message_count} ---\")\n",
    "    message_count += 1\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16021d4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tools in LangChain\n",
    "- LangChain provides a simple way to define and use tools within agents.\n",
    "- You can create a custom tool by tagging a function with the `@tool` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "27f7d09aaf2ca881",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T13:18:10.073864Z",
     "start_time": "2025-11-20T13:18:10.069797Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_current_time() -> str:\n",
    "    \"\"\"Returns the current time in HH:MM format.\"\"\"\n",
    "    return datetime.datetime.now().strftime(\"%H:%M\")\n",
    "\n",
    "@tool\n",
    "def get_current_date() -> str:\n",
    "    \"\"\"Returns the current date in YYYY-MM-DD format.\"\"\"\n",
    "    return datetime.datetime.now().strftime(\"%Y-%m-%d\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94674d88",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Structured Tools\n",
    "- Under the hood, a function tagged with `@tool` is converted into a `StructuredTool`.\n",
    "- A `StructuredTool` requires:\n",
    "    - **Name**\n",
    "    - **Description**\n",
    "    - **`args_schema`**: A Pydantic model defining the arguments the tool expects.\n",
    "- This allows LangChain to automatically generate prompts and parse outputs when using the tool within an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c8ab4d68780bd65b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T13:18:10.123148Z",
     "start_time": "2025-11-20T13:18:10.121293Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredTool(name='get_current_time', description='Returns the current time in HH:MM format.', args_schema=<class 'langchain_core.utils.pydantic.get_current_time'>, func=<function get_current_time at 0x7f983d4f3ba0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_current_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df750a90",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## LangChain Agents\n",
    "- LangChain provides several pre-built agent types that can be easily instantiated and used.\n",
    "- **Under the hood, these agents handle:**\n",
    "    - Deciding when to use tools.\n",
    "    - Formatting prompts.\n",
    "    - Parsing tool outputs.\n",
    "    - *(and much more, but we will focus on these three).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ebe70601818e64ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T13:18:15.874493Z",
     "start_time": "2025-11-20T13:18:10.173901Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Message: 0 ---\n",
      "Message Type: <class 'langchain_core.messages.human.HumanMessage'>\n",
      "Content: What time is it?, tools: None\n",
      "--- Message: 1 ---\n",
      "Message Type: <class 'langchain_core.messages.ai.AIMessage'>\n",
      "Content: , tools: [{'name': 'get_current_time', 'args': {}, 'id': 'call_a99xb21o', 'type': 'tool_call'}]\n",
      "--- Message: 2 ---\n",
      "Message Type: <class 'langchain_core.messages.tool.ToolMessage'>\n",
      "Content: 14:18, tools: None\n",
      "--- Message: 3 ---\n",
      "Message Type: <class 'langchain_core.messages.ai.AIMessage'>\n",
      "Content: The current time is 14:18., tools: []\n"
     ]
    }
   ],
   "source": [
    "from typing import TypeVar, TypedDict\n",
    "from langchain.agents.middleware.types import _InputAgentState, _OutputAgentState\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langchain.agents import create_agent, AgentState\n",
    "C = TypeVar(\"C\")\n",
    "\n",
    "class AgentData(TypedDict):\n",
    "    messages: list[BaseMessage]\n",
    "Agent = CompiledStateGraph[\n",
    "    AgentState[AgentData], C, _InputAgentState, _OutputAgentState[AgentData]\n",
    "]\n",
    "agent: Agent = create_agent(\n",
    "    model=chat,\n",
    "    tools=[get_current_time, get_current_date],\n",
    ")\n",
    "result: AgentData = agent.invoke(input = AgentData(messages=[HumanMessage(content=\"What time is it?\")]))\n",
    "message_count = 0\n",
    "for message in result['messages']:\n",
    "    print(f\"--- Message: {message_count} ---\")\n",
    "    message_count += 1\n",
    "    print(\"Message Type:\", type(message))\n",
    "    print(f\"Content: {message.content}, tools: {getattr(message, 'tool_calls', None)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318a48d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Tool Dilemma\n",
    "- **Problem:** I often need to implement the same tool for multiple AI agents or frameworks.\n",
    "- **Fragmentation:**\n",
    "    - Initially, each library/framework had its own way to define tools.\n",
    "    - OpenAI introduced a standard using JSON Schema.\n",
    "    - Gemini had its own proprietary format.\n",
    "    - Anthropic had another.\n",
    "- **Result:** This fragmentation created significant challenges for developers (the \"N x M\" integration problem)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f577f7e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Towards Standardization\n",
    "- **The Goal:**\n",
    "    - Define a standard way to describe tools (name, description, arguments).\n",
    "    - Use **JSON Schema** for argument definition.\n",
    "    - Create a standard protocol to expose such tools to different AI agents.\n",
    "\n",
    "<img src=\"images/desiderata.png\" alt=\"desiderata\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8a4ab1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MCP: Model Context Protocol\n",
    "> An open standard to define and share **context** across different AI agents and frameworks.\n",
    "- **Why \"Context\" and not just \"Tools\"?**\n",
    "    - While tools are a major part, other resources can be shared too.\n",
    "    - **Resources:** File-like data that can be read by clients (e.g., logs, code files).\n",
    "    - **Prompts:** Pre-defined templates for interacting with the server.\n",
    "    - **Tools:** Executable functions.\n",
    "    - In Agentic AI literature, these are collectively referred to as \"context\".\n",
    "- MCP aims to standardize the definition and sharing of this context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e7a56d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why Standardize?\n",
    "- Just as **REST APIs** standardized web services...\n",
    "- Just as **LSP (Language Server Protocol)** standardized how IDEs talk to language tools...\n",
    "- **MCP** aims to standardize how AI agents and frameworks define and share context.\n",
    "- **Origin:** Anthropic (creators of Claude) introduced the MCP standard to solve the ecosystem fragmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34646924",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MCP Components\n",
    "- **Host:** An entity that **needs** context to operate (e.g., Claude Desktop, IDEs, AI Agents).\n",
    "- **Server:** An entity that **provides** context to hosts (e.g., Google Drive MCP, Postgres MCP).\n",
    "- **Client:** An entity that interacts with the server on behalf of the host and manages the connection (1:1 connection).\n",
    "- *This architecture allows for a \"Many-to-Many\" ecosystem where any Host can talk to any Server.*\n",
    "\n",
    "![](./images/architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c91562",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Overall Interaction\n",
    "![](./images/interaction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e07d730",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### MCP Client - Server Interaction\n",
    "- The **MCP Client** is responsible for:\n",
    "    - Connecting to the MCP Server.\n",
    "    - Requesting context (Resources, Prompts, Tools) on behalf of the Host.\n",
    "    - Handling sampling (server asking the client for completions).\n",
    "- The **MCP Server** is responsible for:\n",
    "    - Exposing context to the MCP Client.\n",
    "    - Handling requests for context.\n",
    "    - Managing tool execution requests.\n",
    "### Layers\n",
    "- **Data Layer (Inner):**\n",
    "  - Defines the **logic** (JSON-RPC 2.0).\n",
    "  - Handles Lifecycle, Primitives (Tools, Resources, Prompts).\n",
    "- **Transport Layer (Outer):**\n",
    "  - Defines the **communication** (Stdio, HTTP).\n",
    "  - Handles connection, message framing.\n",
    "- *Data layer is transport-agnostic.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6f8c8d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MCP LifeCycle\n",
    "- **Stateful Protocol:** MCP requires a persistent connection state.\n",
    "- **Initialization Phase:**\n",
    "  - **Capability Negotiation:** Client and Server exchange supported features (e.g., \"I support resources\", \"I support sampling\").\n",
    "  - **Handshake:** `initialize` request $\\rightarrow$ `initialized` notification.\n",
    "- **JSON-RPC 2.0:** The standard format for all messages.\n",
    "\n",
    "<img src=\"images/lifecycle.png\" alt=\"mcp-lifecycle\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b999dc13",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Primitives & Transport\n",
    "- **Dynamic Discovery:** Clients discover capabilities via `*/list` methods.\n",
    "  - `tools/list`, `resources/list`, `prompts/list`.\n",
    "- **Execution/Retrieval:**\n",
    "  - `tools/call`: Execute a function.\n",
    "  - `resources/read`: Get data content.\n",
    "  - `prompts/get`: Get a template.\n",
    "### Transport Agnosticism\n",
    "- **Decoupled Architecture:** The interaction between Client and Server is independent of the transport mechanism.\n",
    "- **Flexibility:** Clients and Servers operate identically whether over local STDIO or remote HTTP.\n",
    "- **Development Focus:** Developers can build core logic first, then plug in the appropriate transport layer (Stdio, SSE, etc.) based on deployment needs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ada3cbe",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### STDIO Transport\n",
    "- The **STDIO Transport** uses standard input and output streams for communication.\n",
    "- **How it works:**\n",
    "    - The MCP Client and Server read from and write to their respective standard input/output streams.\n",
    "    - Messages are serialized as JSON and exchanged over these streams.\n",
    "- **Use Cases:**\n",
    "    - Local development and testing.\n",
    "    - Integrating MCP Servers as subprocesses in applications.\n",
    "    - I want to do not make my tools available over the network!!\n",
    "\n",
    "### SSE (Server-Sent Events) Transport\n",
    "- **Mechanism:** Uses HTTP for initial connection and Server-Sent Events for server-to-client messages.\n",
    "- **How it works:**\n",
    "    - **Client -> Server:** Standard HTTP POST requests (for sending commands/requests).\n",
    "    - **Server -> Client:** A persistent HTTP connection using SSE (for pushing updates/events).\n",
    "- **Pros:**\n",
    "    - **Remote Access:** Allows connecting to servers running on different machines or cloud environments.\n",
    "    - **Web Compatible:** Works well with standard web infrastructure (proxies, load balancers).\n",
    "- **Use Cases:**\n",
    "    - Cloud-hosted agents accessing local resources via a bridge.\n",
    "    - Distributed systems where agents and tools reside on different nodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b576fa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## OK, but how to develop an MCP Server?\n",
    "- There are multiple SDKs available to help you build MCP Servers quickly.\n",
    "- **Python SDK:** https://github.com/modelcontextprotocol/python-sdk\n",
    "- **Node.js SDK:** https://github.com/modelcontextprotocol/typescript-sdk\n",
    "- **Jave SDK:** https://github.com/modelcontextprotocol/java-sdk\n",
    "- And many other (Go, Rust, ...).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4355aa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## FastMCP\n",
    "- A fast and easy-to-use MCP Server framework in Python.\n",
    "- Simlar to FastAPI for web servers, but for MCP Servers.\n",
    "- Ok, let's make an example for time feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bc53e2378897d982",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T13:18:15.924287Z",
     "start_time": "2025-11-20T13:18:15.922755Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load time-mcp.py\n",
    "from datetime import datetime, timezone\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "mcp = FastMCP(\"Time\")\n",
    "\n",
    "@mcp.tool()\n",
    "async def get_time(timezone: str = \"local\") -> str:\n",
    "    \"\"\"Get current time: 'local' (default) or 'UTC'.\"\"\"\n",
    "    tz = timezone.strip().lower()\n",
    "    now = datetime.now(timezone.utc) if tz == \"utc\" else datetime.now()\n",
    "    return now.isoformat(sep=\" \", timespec=\"seconds\")\n",
    "\n",
    "mcp.run(transport=\"streamable-http\")\n",
    "#mcp.run(transport=\"stdio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813c85bc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Debugging: The MCP Inspector\n",
    "- Developing agents is hard; debugging them is harder.\n",
    "- **MCP Inspector:** A web-based tool to test and inspect MCP Servers.\n",
    "- **Capabilities:**\n",
    "  - List available tools, resources, and prompts.\n",
    "  - Execute tools and view raw JSON-RPC messages.\n",
    "  - **Command:** `npx @modelcontextprotocol/inspector <command-to-run-server>` (or no command to connect to an existing server)\n",
    "\n",
    "<img src=\"images/mcp-inspector.png\" alt=\"mcp-inspector\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59978de",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Using MCP Servers (The Client Side)\n",
    "- **Ready-made Hosts:**\n",
    "  - **Claude Desktop:** Native support for local MCP servers.\n",
    "  - **IDEs:** VS Code (via extensions), Cursor, Zed.\n",
    "- **Programmatic Clients:**\n",
    "  - **LangChain / LangGraph:** Easily integrate MCP tools into custom agents.\n",
    "  - **Custom Scripts:** Use the SDK to build your own client.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a29b276532d6b8",
   "metadata": {},
   "source": [
    "## Live Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4d6db36173de6596",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T13:18:38.193127Z",
     "start_time": "2025-11-20T13:18:15.974481Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:8000/mcp \"HTTP/1.1 200 OK\"\n",
      "Received session ID: ed5e7f8efc02495995d97250fe724bd2\n",
      "Negotiated protocol version: 2025-06-18\n",
      "HTTP Request: GET http://localhost:8000/mcp \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:8000/mcp \"HTTP/1.1 202 Accepted\"\n",
      "HTTP Request: POST http://localhost:8000/mcp \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: DELETE http://localhost:8000/mcp \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:8000/mcp \"HTTP/1.1 200 OK\"\n",
      "Received session ID: e5a0b1578706460fa89e5ab74be2fa44\n",
      "Negotiated protocol version: 2025-06-18\n",
      "HTTP Request: POST http://localhost:8000/mcp \"HTTP/1.1 202 Accepted\"\n",
      "HTTP Request: GET http://localhost:8000/mcp \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:8000/mcp \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:8000/mcp \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: DELETE http://localhost:8000/mcp \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "The current time is **2:18:27 PM** on November 20, 2025.\n"
     ]
    }
   ],
   "source": [
    "from langchain_mcp_adapters.sessions import StreamableHttpConnection\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"time\": StreamableHttpConnection(url = \"http://localhost:8000/mcp\", transport=\"streamable_http\"),\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()  # get tools exposed by the MCP server\n",
    "# Create an agent using the model/chat and the discovered tools\n",
    "agent = create_agent(model=chat, tools=tools)\n",
    "\n",
    "# Invoke the agent with a typed input (AgentData containing messages)\n",
    "result: AgentData = await agent.ainvoke(\n",
    "    AgentData(messages=[HumanMessage(content=\"What time is it??\")])\n",
    ") # ainvoke for async execution (needed for http transport)\n",
    "\n",
    "# Print the final agent message (more readable)\n",
    "print(\"Response:\")\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb56ac9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why This Matters for Us? (Research/Dev)\n",
    "- **Scenario:** Scientific Discovery / Engineering Loop.\n",
    "  - `Code` $\\rightarrow$ `Simulation` $\\rightarrow$ `Analysis` $\\rightarrow$ `Fix`\n",
    "- **Goal:** Automate this loop with Agentic AI.\n",
    "- **MCP's Role:**\n",
    "  - Provides a standard interface to expose our simulators and analysis tools to *any* agent.\n",
    "  - Decouples the \"Tooling\" from the \"Intelligence\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebef7fd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Use Case: Alchemist Simulator\n",
    "- **Context:** A simulator for aggregate computing (Swarm Intelligence).\n",
    "- **MCP Server exposes:**\n",
    "  - `compile(yaml_spec)`: Checks if the simulation spec is valid.\n",
    "  - `simulate(yaml_spec)`: Runs the simulation and returns snapshots/metrics.\n",
    "- **The Agent:** Can iteratively write the spec, fix errors, and analyze results without human intervention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4f50fe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Live demo\n",
    "<img src=\"images/result.png\" alt=\"images/result.png\" width=\"1200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda47cf3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "- **MCP is the \"REST API\" for AI Context.**\n",
    "  - Standardizes how AI models connect to data and tools.\n",
    "- **Solves Fragmentation:** No more proprietary tool definitions.\n",
    "- **Enables an Ecosystem:**\n",
    "  - **Server Devs:** Build tools once, reach all agents.\n",
    "  - **Agent Devs:** Access a massive library of existing tools.\n",
    "- **Future:** Expect MCP to become the default standard for Agentic AI integration.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "rise": {
   "center": true,
   "enable_chalkboard": false,
   "height": "100%",
   "scroll": true,
   "slideNumber": true,
   "theme": "simple",
   "transition": "slide",
   "width": "100%"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
